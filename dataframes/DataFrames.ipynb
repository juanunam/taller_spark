{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "    $(\"#homeButton\").remove()\n",
       "    $('body').append('<a href=\"#'+$(\"h1,h2,h3:eq(0)\").attr(\"id\")+'\" style=\"position: fixed; bottom: 10px; right: 10px;\" type=\"button\" class=\"btn btn-success btn-circle btn-lg\"><i class=\"glyphicon glyphicon-link\" id=\"homeButton\"></i></a>');\n",
       "    $(\"#MainMenu\").remove()\n",
       "    var menu = \n",
       "      '<div id=\"MainMenu\" style=\"position: fixed; top: 120px; right: 10px; z-index:6; max-height: 500px;\">'+\n",
       "        '<div class=\"list-group panel\" >'+\n",
       "          '<a href=\"#collapseMenu\" class=\"list-group-item list-group-item-success\" data-toggle=\"collapse\" data-parent=\"#MainMenu\">Indíce<i class=\"fa fa-caret-down\"></i></a>'+\n",
       "          '<div class=\"collapse\" id=\"collapseMenu\" style=\"overflow-y: overlay; max-height: 500px;\">'+\n",
       "          '</div>'+\n",
       "        '</div>'+\n",
       "      '</div>'\n",
       "   \n",
       "    var parent = $(menu)\n",
       "    var arrayIds = []\n",
       "    $(\"h1,h2,h3\").attr(\"id\",function(id,Value){\n",
       "        if(Value != \"\"){\n",
       "            var content = (Value.replace(new RegExp('-', 'g'), ' '));\n",
       "            content = \"&nbsp;\".repeat(parseInt($(this).prop(\"tagName\")[1])*2-1) + content;\n",
       "            $(parent).find(\"#collapseMenu\").append('<a href=\"#'+Value+'\" style=\"position:relative;\" class=\"list-group-item\" data-parent=\"#SubMenu1\">'+content+'</a>');\n",
       "        }\n",
       "    })\n",
       "$('body').append(parent)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "    $(\"#homeButton\").remove()\n",
    "    $('body').append('<a href=\"#'+$(\"h1,h2,h3:eq(0)\").attr(\"id\")+'\" style=\"position: fixed; bottom: 10px; right: 10px;\" type=\"button\" class=\"btn btn-success btn-circle btn-lg\"><i class=\"glyphicon glyphicon-link\" id=\"homeButton\"></i></a>');\n",
    "    $(\"#MainMenu\").remove()\n",
    "    var menu = \n",
    "      '<div id=\"MainMenu\" style=\"position: fixed; top: 120px; right: 10px; z-index:6; max-height: 500px;\">'+\n",
    "        '<div class=\"list-group panel\" >'+\n",
    "          '<a href=\"#collapseMenu\" class=\"list-group-item list-group-item-success\" data-toggle=\"collapse\" data-parent=\"#MainMenu\">Indíce<i class=\"fa fa-caret-down\"></i></a>'+\n",
    "          '<div class=\"collapse\" id=\"collapseMenu\" style=\"overflow-y: overlay; max-height: 500px;\">'+\n",
    "          '</div>'+\n",
    "        '</div>'+\n",
    "      '</div>'\n",
    "   \n",
    "    var parent = $(menu)\n",
    "    var arrayIds = []\n",
    "    $(\"h1,h2,h3\").attr(\"id\",function(id,Value){\n",
    "        if(Value != \"\"){\n",
    "            var content = (Value.replace(new RegExp('-', 'g'), ' '));\n",
    "            content = \"&nbsp;\".repeat(parseInt($(this).prop(\"tagName\")[1])*2-1) + content;\n",
    "            $(parent).find(\"#collapseMenu\").append('<a href=\"#'+Value+'\" style=\"position:relative;\" class=\"list-group-item\" data-parent=\"#SubMenu1\">'+content+'</a>');\n",
    "        }\n",
    "    })\n",
    "$('body').append(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    h3{\n",
       "        color:#0094E4;\n",
       "    }\n",
       "    h2{\n",
       "         color:#F28773;\n",
       "    }\n",
       "    h1{\n",
       "         color:#0094E4;\n",
       "    }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    h3{\n",
    "        color:#0094E4;\n",
    "    }\n",
    "    h2{\n",
    "         color:#F28773;\n",
    "    }\n",
    "    h1{\n",
    "         color:#0094E4;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/logo.png\" style=\"width: 20%; margin-left: 70%;\">\n",
    "<img src=\"./images/python.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames\n",
    "\n",
    "En Spark 2.0 y otras versiones más recientes los DataFrames son el \"caballito de batalla\" y la forma principal de trabajar con Python y Spark en conjunto.\n",
    "\n",
    "<div class=\"alert alert-info\">Los <strong>DataFrames</strong> funcionan como tablas o DataFrames de pandas, ya que se comportan como si tuvieran filas y columnas. En realidad son filas distribuidas.</div>\n",
    "\n",
    "Usar **DataFrames** proporciona varias ventajas:\n",
    "* Sintaxis más simple que la de los RDDs\n",
    "* Posibilidad de usar comandos tipo SQL directamente sobre el DataFrame\n",
    "* Las operaciones están automáticamente distribuidas en los RDDs\n",
    " \n",
    "La principal ventaja de usar los DataFrames de Spark sobre Pandas o cualquier otra cosa es que Spark puede manipular datos a través de muchos RDDs, cantidades masivas de datos que nocaben en una sola computadora. El costo por esto es una sintaxis un poquito rara, pero nada terrible.\n",
    "\n",
    "## Para crear un DataFrame\n",
    "\n",
    "Primero necesitamos importar una SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initiallizing Spark (just run once) https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-sparkcontext-creating-instance-internals.html\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\">La SparkSession nos permite accesar a todas las funcionalidades de Spark (incluyendo Data frames), mientras que el Spark Context no incluye DataFrames.</div>\n",
    "\n",
    "Luego, inicializar la SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Puede que tome un tiempo en una computadora local\n",
    "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear un data frae necesitamos alguna de las siguientes opciones:\n",
    "* Obtener datos de un archivo.\n",
    "* Conectarnos a un archivo grande y distribuido (como HDFS).\n",
    "* Convertir un RDD en DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Vamos a empezar con cómo leer desde un archivo\n",
    "\n",
    "df = spark.read.json('./data/people2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo sé qué otros métodos hay para mi DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  Para DataFrames no tenemos <strong>isEmpty</strong> ni <strong>countApprox</strong>, por lo que si queremos conocer si contiene datos nuestroDataFrame es recomendable convertirlo en RDD primero. En general no es buena idea hacer un <strong>collect</strong> o un <strong>count</strong> en DataFrames cuya magnitud no conocemos.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.rdd.isEmpty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esquema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En algunos tipos de datos como los formatos tabulares de csv, es muy fácil inferir un esquema, pero es posible que sea necesario definir el esquema manualmente si se planea utilizar un métodt .read que no tenga la capacidad de inferir un esquema (inferSchema)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark cuenta con las herramientas para realizar lo anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField,StringType,IntegerType,StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que cargamos las herramientas necesitamos crear la lista de los campos estructurados (StructFields): \n",
    "* :param name: string, nombre del campo.\n",
    "* :param dataType: :class:`DataType` del campo.\n",
    "* :param nullable: boolean, Si puede ser nulo (None) o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_schema = [StructField(\"age\", IntegerType(), True),StructField(\"name\", StringType(), True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el esquema por completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_struc = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y por último asignamos el esquema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json('people.json', schema=final_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccionar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(df['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.select('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(df.select('age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns list of Row objects\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionar múltiples columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.select(['age','name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.select(['age','name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear nuevas columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Agregar una nueva columna que es una copia de otra\n",
    "df.withColumn('newage',df['age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Renombrar una columna\n",
    "df.withColumnRenamed('age','supernewage').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algunas operaciones ligeramente más complicadas para agregar una columna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.withColumn('doubleage',df['age']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.withColumn('add_one_age',df['age']+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.withColumn('half_age',df['age']/2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.withColumn('half_age',df['age']/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('./data/appl_stock.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El filtrado funciona igual que en el caso de los RDDs, nos devuelve los datos que cumplan con cierta condición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.filter(df.Close < 500).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.filter(df.Close < 500).describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Otro modo de hacerlo\n",
    "df.filter(\"Close < 500\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede ser que yo no quiera ver todo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.filter(\"Close < 500\").select(['Open','Close']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, entonces, ¿qué hacen los siguientes comandos de spark con nuestro DF original?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.filter((df['Close'] < 200) & (df['Open'] > 200)).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.filter((df['Close'] < 200) & ~(df['Open'] > 200)).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.filter((df['Low'] == 197.16)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podemos guardar información que nos interese particularmente en un diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = df.filter((df['Low'] == 197.16)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row.asDict()['Volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groupBy y aggregate\n",
    "* groupBy: agrupa datos juntos con base en los valores de alguna columna\n",
    "* aggregate: combines aggregates combina y/o agrega en una sola salida (como las sumas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('./data/sales_info.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.limit(6).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"Company\").mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"Company\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"Company\").max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregate\n",
    "\n",
    "<div class=\"alert alert-info\"> Cuando no agregamos GroupBy, solamente estamos agregando a través de todo </div>\n",
    "\n",
    "<div class=\"alert alert-info\"> Una de las formas de agregar es usar un diccionario como entrada </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.agg({'Sales':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.agg({'Sales':'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con grupo de agregación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_data = df.groupBy(\"Company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_data.agg({'Sales':'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay más fucnciones de agregación como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct,avg,stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "### Cargar el Walmart stock CSV File, con Saprk infiriendo el esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('./data/walmart_stock.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo se llaman las columnas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo se ve el esquema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo imprimirían las primeras cinco líneas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo usamos describe para aprender del DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La media y la desviación estándar nos muestran muchooos puntos decimales. Podemos formatear los números para mostrarnos tan sólo hasta dos decimales. \n",
    "\n",
    "(Hint, primero hay que chear el esquema de describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "result = df.describe()\n",
    "result2 = result.select(result['summary'],\n",
    "              format_number(result['Open'].cast('float'),2).alias('Open'),\n",
    "              format_number(result['High'].cast('float'),2).alias('High'),\n",
    "              format_number(result['Low'].cast('float'),2).alias('Low'),\n",
    "              format_number(result['Close'].cast('float'),2).alias('Close'),\n",
    "              result['Volume'].cast('int').alias('Volume')\n",
    "             )\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crea un nuevo DataFrame con una columna que se llame HV Ratio que sea el cociente de el High Prices vs el volumen del stock (Volume)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.select(\n",
    "    (df.High/df.Volume).alias(\"HV Ratio\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué día fue el pico más alto en High?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Didn't need to really do this much indexing\n",
    "# Could have just shown the entire row\n",
    "df.orderBy(df[\"High\"].desc()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.orderBy(df[\"High\"].desc()).head(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cuál es la media de la columna Close?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col as c\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.select(\n",
    " F.avg(df[\"Close\"].cast('Float')).cast('Float').alias(\"MEAN_Close\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.select(F.mean(\"Close\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cuáles son el máximo y el mínimo de la columna Volume?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.select(\n",
    "    F.max(\"Volume\"),\n",
    "    F.min(\"Volume\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cuál fue el máximo High al año?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df.select('*',\n",
    "          F.year(\"Date\").alias('year'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.groupBy('year').max('High').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursos\n",
    "- [About Spark in mapr](https://mapr.com/blog/spark-101-what-it-what-it-does-and-why-it-matters/)\n",
    "- [edX course Big Data analytics Using Spark](https://courses.edx.org/courses/course-v1:UCSanDiegoX+DSE230x+1T2018/courseware/2966295eefb4492b836de9ee34f05911/56608329f8fc42fc8fcaa5e49673ee3b/5?activate_block_id=block-v1%3AUCSanDiegoX%2BDSE230x%2B1T2018%2Btype%40vertical%2Bblock%4042ff48e505e24071b920a496b48a32bc)\n",
    "- [paper regarding RDDs](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)\n",
    "- [saurzcode on what is RDD](https://saurzcode.in/2015/10/what-is-rdd-in-spark-and-why-do-we-need-it/)\n",
    "- [Spark-py notebooks by Jose A. Dianes](https://github.com/jadianes/spark-py-notebooks)\n",
    "- [Presenting code using Jupyter Notebook Slides](https://medium.com/@mjspeck/presenting-code-using-jupyter-notebook-slides-a8a3c3b59d67)\n",
    "- http://dme.rwth-aachen.de/en/research/projects/mapreduce\n",
    "- https://softwareengineering.stackexchange.com/questions/98800/is-mapreduce-anything-more-than-just-an-application-of-divide-and-conquer\n",
    "- http://data-flair.training/forums/topic/sparksession-vs-sparkcontext-in-apache-spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
