{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "    $(\"#homeButton\").remove()\n",
    "    $('body').append('<a href=\"#'+$(\"h1,h2,h3:eq(0)\").attr(\"id\")+'\" style=\"position: fixed; bottom: 10px; right: 10px;\" type=\"button\" class=\"btn btn-success btn-circle btn-lg\"><i class=\"glyphicon glyphicon-link\" id=\"homeButton\"></i></a>');\n",
    "    $(\"#MainMenu\").remove()\n",
    "    var menu = \n",
    "      '<div id=\"MainMenu\" style=\"position: fixed; top: 120px; right: 10px; z-index:6; max-height: 500px;\">'+\n",
    "        '<div class=\"list-group panel\" >'+\n",
    "          '<a href=\"#collapseMenu\" class=\"list-group-item list-group-item-success\" data-toggle=\"collapse\" data-parent=\"#MainMenu\">Indíce<i class=\"fa fa-caret-down\"></i></a>'+\n",
    "          '<div class=\"collapse\" id=\"collapseMenu\" style=\"overflow-y: overlay; max-height: 500px;\">'+\n",
    "          '</div>'+\n",
    "        '</div>'+\n",
    "      '</div>'\n",
    "   \n",
    "    var parent = $(menu)\n",
    "    var arrayIds = []\n",
    "    $(\"h1,h2,h3\").attr(\"id\",function(id,Value){\n",
    "        if(Value != \"\"){\n",
    "            var content = (Value.replace(new RegExp('-', 'g'), ' '));\n",
    "            content = \"&nbsp;\".repeat(parseInt($(this).prop(\"tagName\")[1])*2-1) + content;\n",
    "            $(parent).find(\"#collapseMenu\").append('<a href=\"#'+Value+'\" style=\"position:relative;\" class=\"list-group-item\" data-parent=\"#SubMenu1\">'+content+'</a>');\n",
    "        }\n",
    "    })\n",
    "$('body').append(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    h3{\n",
    "        color:#0094E4;\n",
    "    }\n",
    "    h2{\n",
    "         color:#F28773;\n",
    "    }\n",
    "    h1{\n",
    "         color:#0094E4;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/logo.png\" style=\"width: 40%; margin-left: 50%;\">\n",
    "<img src=\"./images/spark.png\">\n",
    "\n",
    "# Tipos de modelos y regresión lineal simple\n",
    "En ciencia de datos o machine learning deseamos hacer programas que puedan aprender de los datos.\n",
    "\n",
    "Una definición más formal y orientada a la ingeniería es:\n",
    "\n",
    "<div class=\"alert alert-info\">Se dice que un <strong>programa de computadora aprende de la experiencia E con respecto a una tarea T y alguna medida de desempeño P, si su desempeño en T, como medida de P, mejora con al experiencia E</strong>.</div>\n",
    "\n",
    "Para aprender, nuestro programa usará un conjunto de datos llamados de \"entrenamiento\" o *training*. Este conjunto de datos es la experiencia **E** que le brindamos al programa. Un ejemplo de tarea **T** puede ser clasificar correos como spam o ham y la medida de desempeño podría ser el cociente de correos correctamente clasificados.\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  Si bajamos una copia de Wikipedia a nuestra computadora tendremos más datos en ella, pero la computadora no se habrá vuelto mejor en ninguna tarea por lo que no se le puede considerar a esto Machine Learning ni Ciencia de Datos.\n",
    "</div>\n",
    "\n",
    "En general, usar técnicas de Machine Learning nos permite crear programas más pequeños, fáciles de mantener y que es más probable que nos den resultados correctos.\n",
    "\n",
    "## Los tipos de modelos que se usan en ciencia de datos \n",
    "\n",
    "\n",
    "En general se clasifican, de acuerdo a la cantidad de supervisión que llevan durante el entrenamiento, en:\n",
    "* **Supervisados**: Los datos de entrenamiento con los que alimentamos el algoritmo incluyen la solución deseada, llamada target (blanco) o label (etiqueta).\n",
    "\n",
    "\n",
    "<img src=\"./images/training_sup.png\" style=\"width: 60%; margin-left: 10%;\">\n",
    "\n",
    "* **No supervisados**: Los datos no están marcados (vienen sin etiqueta) y el sistema intenta aprender sin maestro.\n",
    "\n",
    "<img src=\"./images/training_nosup.png\" style=\"width: 85%; margin-left: 10%;\">\n",
    "\n",
    "## ¿Para qué usamos los modelos en ciencia de datos?\n",
    "\n",
    "\n",
    "<img src=\"./images/class_vs_reg.png\" style=\"width: 85%; margin-left: 10%;\">\n",
    "\n",
    "* **Clasificación:** Queremos mapear entradas a etiquetas salida\n",
    "* **Regresión:** Queremos mapear una entrada a una salida continua. \n",
    "\n",
    "\n",
    "# Regresión lineal\n",
    "\n",
    "Un modelo lineal hace una predicción computando una suma ponderada de las variables predictoras (input features), más una constante a la que se le llama el \"término de sesgo\" o término de intercepción.\n",
    "\n",
    "$\\hat{y}=\\theta_{0}+\\theta_{0}x_1+\\theta_{2}x_2+...+\\theta_{n}x_n$     \n",
    "\n",
    "Donde:\n",
    "\n",
    "* $\\hat{y}:$ Es la predicción\n",
    "* $n:$ Es el número de variables o características.\n",
    "* $x_i:$ Es la iésima variable o característica.\n",
    "* $\\theta_j:$ Es el jotaésimo parámetro del modelo\n",
    "\n",
    "Esta se puede expresar también de este modo:\n",
    "\n",
    "$\\hat{y}=\\theta^{T} \\cdot \\mathbf{x}$     \n",
    "\n",
    "Donde:\n",
    "\n",
    "* $\\theta:$ Es el vector de parámetros del modelo\n",
    "* $\\theta^{T}:$ Es el vector **traspuesto** de parámetros del modelo\n",
    "* $\\mathbf{x}:$ Es el vector de las variables   \n",
    "* $\\theta^{T} \\cdot \\mathbf{x}:$ Es el producto punto de $\\theta^{T}$ y $\\mathbf{x}$\n",
    "\n",
    "Por lo general la P (medida de desempeño) que se usa aquí es el error cuadrado medio.\n",
    "\n",
    "$ECM = \\frac{1}{m}\\sum_{i=0}^m(\\theta^T \\cdot x^{(i)}-y^{(i)})^2$\n",
    "\n",
    "\n",
    "<img src=\"images/lineal_r_simple.png\">\n",
    "\n",
    "# Para verlo más claramente tomemos un problema:\n",
    "\n",
    "https://www.kaggle.com/c/boston-housing/rules\n",
    "\n",
    "Quiero conocer el valor medio de casa ocupadas por sus dueños en miles de dólares (**medv**) en los suburbios de Boston.\n",
    "\n",
    "Mis variables (features, covariables,etc...) que represento como columnas son:\n",
    "\n",
    "**crim**\n",
    "tasa de crímenes *per capita* por ciudad.\n",
    "\n",
    "**zn**\n",
    "proporción de de tierras residenciales por lotes de 25,000 sq.ft.\n",
    "\n",
    "**indus**\n",
    "proportion of non-retail business acres per town.\n",
    "\n",
    "**chas**\n",
    "Charles River dummy variable (= 1 si estamos cerca del río; 0 e caos contrario).\n",
    "\n",
    "**nox**\n",
    "concentración de óxido de nitrógeno (partes por 10 millones).\n",
    "\n",
    "**rm**\n",
    "Número promedio de recámaras por casa.\n",
    "\n",
    "**age**\n",
    "proportion of owner-occupied units built prior to 1940.\n",
    "\n",
    "**dis**\n",
    "weighted mean of distances to five Boston employment centres.\n",
    "\n",
    "**rad**\n",
    "index of accessibility to radial highways.\n",
    "\n",
    "**tax**\n",
    "full-value property-tax rate per \\$10,000.\n",
    "\n",
    "**ptratio**\n",
    "pupil-teacher ratio by town.\n",
    "\n",
    "**black**\n",
    "1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n",
    "\n",
    "**lstat**\n",
    "lower status of the population (percent).\n",
    "\n",
    "**medv**\n",
    "median value of owner-occupied homes in \\$1000s.\n",
    "\n",
    "\n",
    "## Cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sc= SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "house_df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('./data/boston.csv')\n",
    "house_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptivos de los datos (lo primero que debemos hacer siempre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "house_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "house_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "house_df.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supuestos de la regresión lineal\n",
    "\n",
    "* **Linealidad:** Tus valores de $x$ y $y$ tienen una relación aproximadamente lineal.\n",
    "* **Independencia:** Las covariables (features) $x_i$ y $x_j$ deben ser independientes para toda i ≠ j.\n",
    "\n",
    "Dados los supuestos anteriores me gustaría ver si mis variables dependen unas de otras, por lo que hago una matriz de *scatter plots* (gráfica de dispersión)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "numeric_features = [t[0] for t in house_df.dtypes if t[1] == 'int' or t[1] == 'double']\n",
    "sampled_data = house_df.select(numeric_features).sample(False, 0.8).toPandas()\n",
    "axs = pd.scatter_matrix(sampled_data, figsize=(10, 10))\n",
    "n = len(sampled_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra cosa que podemos hacer es hallar la correlación entre las covariables y el target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import six\n",
    "for i in house_df.columns:\n",
    "    if not( isinstance(house_df.select(i).take(1)[0][0], six.string_types)):\n",
    "        print( \"Correlación con medv para \", i, house_df.stat.corr('medv',i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  En el deber ser, tendríamos que filtrar todas las covariables que tienen una dependencia entre sí o combinarlas en una sola variable.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De momento dejaremos los datos como están."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando los datos para hacer nuestro modelo en Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols = ['crim', 'zn',\\\n",
    "    'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',\\\n",
    "    'ptratio', 'black', 'lstat'], outputCol = 'features')\n",
    "vhouse_df = vectorAssembler.transform(house_df)\n",
    "vhouse_df = vhouse_df.select(['features', 'medv'])\n",
    "vhouse_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hago mis subconjuntos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splits = vhouse_df.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora sí (por fin!) hago mi regresión lineal.\n",
    "\n",
    "### 1.Entreno mi modelo con el subconjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='medv', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veo mis coeficientes ($\\theta$'s) y mi \"intercept\" $\\theta_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dados mis datos de entrenamiento, ¿cómo salió? Recordemos que para eso tengo mi criterio de error cuadrado medio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"ECM: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prize time!!!!\n",
    "\n",
    "\n",
    "<img src=\"./images/prize.jpeg\">\n",
    "\n",
    "\n",
    "¿Qué es $r^2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo se ve muy bien :D\n",
    "    \n",
    "Pero...\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  El desempeño con mi conjunto de datos de entrenamiento puede no reflejar qué tan bien predice nuestro modelo lo que pasará con la población total o con otros conjunto de datos.\n",
    "</div>\n",
    "\n",
    "Entonces, usamos el conjutno de datos de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Probamos con el conjunto de datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"medv\",\"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluamos cómo quedó la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"medv\",metricName=\"r2\")\n",
    "print(\"R cuadrada (r2) en datos de prueba = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"medv\",metricName='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"ECM en datos de prueba = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ahora, si tuvieramos otro conjunto de datos, el siguiente paso sería hacer predicciones con mi modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(test_df)\n",
    "predictions.select(\"prediction\",\"medv\",\"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuentes\n",
    "- https://www.kdnuggets.com/2018/04/supervised-vs-unsupervised-learning.html\n",
    "- Hands-On Machine Learning with Scikit-Learn & Tensor Flow Auréliene Géron  \n",
    "- https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a\n",
    "- https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#linear-regression\n",
    "- https://medium.com/@mjspeck/a-guide-to-basic-modeling-techniques-properties-b2a88841e218"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
